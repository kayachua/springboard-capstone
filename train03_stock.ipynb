{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b2e3c2a-4526-464d-8439-388e9612f716",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "import transformers, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2499bba-7ec2-43a1-ac2c-a774dab7ea0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "for device in physical_devices:\n",
    "    tf.config.experimental.set_memory_growth(device, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8633d7c5-3d7a-403e-980c-0806766e6372",
   "metadata": {},
   "source": [
    "In notebook 1, we trained the BERTweet transformer model on the tweet_eval sentiment dataset (saved as 'bertweet_simple'). \n",
    "In notebook 2, we further fine-tuned by re-training that model on a specialized dataset consisting of tweet data related to\n",
    "Apple ('bertweet_aapl'). In this notebook, we'll repeat the process by re-training the simple model on an amalgamation of three\n",
    "labeled datasets: the AAPL data as in notebook 2, plus the Sanders dataset and a collection of tweets dated to 2020, both related\n",
    "to stock performance. (The training pipeline is parallel to the one used in notebook 2.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34659e7a-3a38-4f29-9712-b00eeb52d8ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-10917645a3750407\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to C:\\Users\\Kaya\\.cache\\huggingface\\datasets\\csv\\default-10917645a3750407\\0.0.0\\433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe907b9ae99a4af8b5f81a13624943bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a50faea937ee4be0883390de113f1338",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to C:\\Users\\Kaya\\.cache\\huggingface\\datasets\\csv\\default-10917645a3750407\\0.0.0\\433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddf5ccc9f9d9485e9b435e55f2fcb7b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 7704\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Features, Value, ClassLabel, load_dataset\n",
    "\n",
    "features = Features({\n",
    "    'text': Value(dtype='string'),\n",
    "    'label': ClassLabel(num_classes=3, names=['negative', 'neutral', 'positive'])})\n",
    "\n",
    "stock_tweet_dataset = load_dataset(\n",
    "    path='csv', \n",
    "    data_files=['./data/aapl.csv', './data/sanders.csv', './data/tweets_labeled_20200904.csv'], \n",
    "    features=features)\n",
    "\n",
    "stock_tweet_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "264ae769-9ec2-437c-983f-11503cf5ebc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': '@apple and @facebook I speak for all of humanity; We want to chose what contacts are added to our contacts book from #facebook #thanks',\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock_tweet_dataset['train'][50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2fefdd8b-c35b-4c2b-b280-108eda013e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into a training set and a combined test+validation set\n",
    "stock_tweet_dataset_split_1 = stock_tweet_dataset['train'].train_test_split(test_size=0.2, shuffle=True, seed=42) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0bdb5017-c11c-4ad0-9290-3ad06ad74f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split test+validation data \n",
    "stock_tweet_dataset_split_2 = stock_tweet_dataset_split_1['test'].train_test_split(test_size=0.7, shuffle=True, seed=84)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9871bb8-5efc-4858-b6c0-e5cca070bfcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 6163\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 1079\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 462\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "stock_tweet_dataset_split_full = DatasetDict({\n",
    "    'train':      stock_tweet_dataset_split_1['train'],\n",
    "    'test':       stock_tweet_dataset_split_2['test'],\n",
    "    'validation': stock_tweet_dataset_split_2['train']})\n",
    "\n",
    "stock_tweet_dataset_split_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5359ed5b-f2a7-442f-8ae1-519cada38aaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'I am looking to #invest in #stocks and I want to invest in a #blackowned #tech or #sustainablefashion company. If you have any recommendations, please let me know ðŸ™ðŸ¾ðŸ’•',\n",
       " 'label': 2}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock_tweet_dataset_split_full['train'][50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05b55f02-d674-48d2-b86c-f851ad0101ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3617620-7766-48cf-992e-6bd222d4d68d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [0, 47, 1600, 3345, 9646, 13545, 141, 6, 2307, 10638, 4, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer(\"The quick brown fox jumps over the lazy dog.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bfcf2e42-74f8-43a0-990b-fec777c9f85b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': Value(dtype='string', id=None),\n",
       " 'label': ClassLabel(num_classes=3, names=['negative', 'neutral', 'positive'], id=None)}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock_tweet_dataset_split_full['train'].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "741ccedf-d12f-41a5-bc27-dc3cc54d93c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function tokenize_fn at 0x000001AC01B75940> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13f6f148679b4f39a5107cb06c41a2b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0ex [00:00, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7255440146ce43a482b52093b901e880",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0ex [00:00, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2988aca8de1c44b89454a08aedc0a89d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0ex [00:00, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_fn(examples):\n",
    "    return tokenizer(examples['text'], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_stock_tweet = stock_tweet_dataset_split_full.map(tokenize_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bea97957-3663-42e5-95ed-d3c6362772ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.data.data_collator import tf_default_data_collator\n",
    "\n",
    "data_collator = tf_default_data_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9490f0d0-2719-4d00-9623-07939a003516",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_train_dataset = tokenized_stock_tweet['train'].to_tf_dataset(\n",
    "    columns=[\"attention_mask\", \"input_ids\", \"token_type_ids\"],\n",
    "    label_cols=[\"label\"],\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=8,\n",
    ")\n",
    "\n",
    "tf_validation_dataset = tokenized_stock_tweet['validation'].to_tf_dataset(\n",
    "    columns=[\"attention_mask\", \"input_ids\", \"token_type_ids\"],\n",
    "    label_cols=[\"label\"],\n",
    "    shuffle=False,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=8,\n",
    ")\n",
    "\n",
    "tf_test_dataset = tokenized_stock_tweet['test'].to_tf_dataset(\n",
    "    columns=[\"attention_mask\", \"input_ids\", \"token_type_ids\"],\n",
    "    label_cols=[\"label\"],\n",
    "    shuffle=False,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b673bcd8-6fa3-4f73-93ce-29f0efd83d29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at C:/Users/Kaya/Documents/capstone/models/bertweet_simple/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFAutoModelForSequenceClassification\n",
    "\n",
    "bertweet_simple = TFAutoModelForSequenceClassification.from_pretrained(\"./models/bertweet_simple/\", num_labels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "032d8704-6ece-4e04-93ee-a457c9ef8a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import create_optimizer\n",
    "\n",
    "batch_size = 8\n",
    "num_epochs = 3\n",
    "batches_per_epoch = len(tf_train_dataset) // batch_size\n",
    "total_train_steps = int(batches_per_epoch * num_epochs)\n",
    "optimizer, schedule = create_optimizer(init_lr=5e-6, num_warmup_steps=0, num_train_steps=total_train_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4d4c1ade-704a-42c1-92da-0e6d46d6c267",
   "metadata": {},
   "outputs": [],
   "source": [
    "bertweet_simple.compile(\n",
    "    optimizer = optimizer,\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics = [tf.metrics.SparseCategoricalAccuracy()],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "587bbc36-f3a3-471b-a3c5-6f81d67c1848",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Kaya\\anaconda3\\envs\\capstone\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py:5043: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n",
      "135/135 [==============================] - 16s 75ms/step - loss: 0.9094 - sparse_categorical_accuracy: 0.6534\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.909382700920105, 0.6533827781677246]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bertweet_simple.evaluate(tf_test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "34221dfb-981d-4d1d-b9dd-ea3eb3023bb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "770/770 [==============================] - 206s 250ms/step - loss: 0.6418 - sparse_categorical_accuracy: 0.7269 - val_loss: 0.5860 - val_sparse_categorical_accuracy: 0.7511\n",
      "Epoch 2/3\n",
      "770/770 [==============================] - 193s 250ms/step - loss: 0.6105 - sparse_categorical_accuracy: 0.7420 - val_loss: 0.5860 - val_sparse_categorical_accuracy: 0.7511\n",
      "Epoch 3/3\n",
      "770/770 [==============================] - 194s 251ms/step - loss: 0.6081 - sparse_categorical_accuracy: 0.7417 - val_loss: 0.5860 - val_sparse_categorical_accuracy: 0.7511\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1acb97ee940>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bertweet_stock_tweet = bertweet_simple\n",
    "\n",
    "bertweet_stock_tweet.fit(tf_train_dataset, validation_data=tf_validation_dataset, epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "376376cb-5a81-49cb-9dda-6ea259ad5b03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135/135 [==============================] - 11s 78ms/step - loss: 0.6168 - sparse_categorical_accuracy: 0.7331\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6167556047439575, 0.7330861687660217]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bertweet_stock_tweet.evaluate(tf_test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6005c47c-8639-4eb9-9c01-519b0565535a",
   "metadata": {},
   "source": [
    "The 'bertweet_simple' model achieved 0.6533 accuracy on the test split of the combined data, while our newly trained \n",
    "model 'bertweet_stock_tweet' performed at 0.7331. This is a similar jump in accuracy as we saw in the AAPL-specific\n",
    "model on a larger and slightly more heterogeneous dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "df5fcf61-cf4d-403e-b1ec-aaf7f165bd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bertweet_stock_tweet.save_pretrained(\"./models/bertweet_stock_tweet/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4729ef9e-a960-49de-9713-8bd9ccc3e8ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-06c6b7501b5e7d6e\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to C:\\Users\\Kaya\\.cache\\huggingface\\datasets\\csv\\default-06c6b7501b5e7d6e\\0.0.0\\433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5514a9b2aa0e4f5cad9e40924d36b367",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1228e24862bd4b31888339bf639b445f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to C:\\Users\\Kaya\\.cache\\huggingface\\datasets\\csv\\default-06c6b7501b5e7d6e\\0.0.0\\433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6bd250697a3488dafe33238482571e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 2980\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aapl_dataset = load_dataset(\n",
    "    path='csv', \n",
    "    data_files=['./data/aapl.csv'], \n",
    "    features=features)\n",
    "\n",
    "aapl_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6d32c932-5ffc-4e09-935f-d96c9ecd9932",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2936d234731249c3896a8114ee05f9e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0ex [00:00, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_aapl_dataset = aapl_dataset.map(tokenize_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "360d3dfc-f79b-41a5-b5cb-75f145fb40c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_aapl_dataset = tokenized_aapl_dataset['train'].to_tf_dataset(\n",
    "    columns=[\"attention_mask\", \"input_ids\", \"token_type_ids\"],\n",
    "    label_cols=[\"label\"],\n",
    "    shuffle=False,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6dcda5c4-65f9-4a1a-820d-0e3523ab8a01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "373/373 [==============================] - 28s 76ms/step - loss: 0.4826 - sparse_categorical_accuracy: 0.8050\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4825942814350128, 0.8050335645675659]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bertweet_stock_tweet.evaluate(tf_aapl_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:capstone]",
   "language": "python",
   "name": "conda-env-capstone-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
