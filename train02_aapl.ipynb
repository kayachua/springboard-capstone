{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5d59360-066b-4983-bfc0-e4a37a0d5b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "import transformers, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce569542-ad3c-4e09-89e9-3706c4c29a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "for device in physical_devices:\n",
    "    tf.config.experimental.set_memory_growth(device, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2868fba-d12c-4406-a830-55b7a4e08caa",
   "metadata": {},
   "source": [
    "In the previous notebook, we used the Hugging Face libraries to load the BERTweet transformer model and train it\n",
    "on the tweet_eval sentiment dataset. Now we'll take that model and train it on a more specialized datasets. Here that\n",
    "will be a small-ish labeled dataset consisting of tweets related to Apple (APPL). The labels again come in three classes\n",
    "indicating positive, negative, and neutral sentiment.\n",
    "\n",
    "Rather than importing the data directly from Hugging Face, it will be taken from a csv file, so we'll have to take care\n",
    "to load it into the same object type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04fbe5a1-9879-482e-9e31-6e7966532e3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-06c6b7501b5e7d6e\n",
      "Reusing dataset csv (C:\\Users\\Kaya\\.cache\\huggingface\\datasets\\csv\\default-06c6b7501b5e7d6e\\0.0.0\\433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21760df2d9f64d16beb03f5c9422e4e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 2980\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Features, Value, ClassLabel, load_dataset\n",
    "\n",
    "features = Features({\n",
    "    'text': Value(dtype='string'),\n",
    "    'label': ClassLabel(num_classes=3, names=['negative', 'neutral', 'positive'])})\n",
    "\n",
    "aapl_dataset = load_dataset(path='csv', data_files=['./data/aapl.csv'], features=features)\n",
    "\n",
    "aapl_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3fff930a-d797-47ec-a348-d14d28ca2600",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': '@apple and @facebook I speak for all of humanity; We want to chose what contacts are added to our contacts book from #facebook #thanks',\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aapl_dataset['train'][50]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d2f017-dbd8-4057-862e-dcee696364dd",
   "metadata": {},
   "source": [
    "As we can see, the load_dataset() function has created a DatasetDict object. Unlike the previous case, here the data\n",
    "was not divided into splits. At the moment, aapl_dataset contains a single Dataset object called 'train'. Let's create\n",
    "training, validation, and test splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db36d48a-c35d-46f7-a191-7da288a34ba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached split indices for dataset at C:\\Users\\Kaya\\.cache\\huggingface\\datasets\\csv\\default-5610044f9daaa1b4\\0.0.0\\433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519\\cache-6b51b7dc9839905b.arrow and C:\\Users\\Kaya\\.cache\\huggingface\\datasets\\csv\\default-5610044f9daaa1b4\\0.0.0\\433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519\\cache-ecec47d30393e3ed.arrow\n"
     ]
    }
   ],
   "source": [
    "# split data into a training set and a combined test+validation set\n",
    "aapl_dataset_split_1 = aapl_dataset['train'].train_test_split(test_size=0.2, shuffle=True, seed=42) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1deb0d81-49ba-42c7-9bb1-ffa8d4fe8ae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached split indices for dataset at C:\\Users\\Kaya\\.cache\\huggingface\\datasets\\csv\\default-5610044f9daaa1b4\\0.0.0\\433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519\\cache-eb3f9e6e19fa32d2.arrow and C:\\Users\\Kaya\\.cache\\huggingface\\datasets\\csv\\default-5610044f9daaa1b4\\0.0.0\\433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519\\cache-a0ae947f87b867e5.arrow\n"
     ]
    }
   ],
   "source": [
    "# split test+validation data \n",
    "aapl_dataset_split_2 = aapl_dataset_split_1['test'].train_test_split(test_size=0.7, shuffle=True, seed=84)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1734fe1-aea9-423c-a3fb-6a5ed69bf038",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 2384\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 418\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 178\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "aapl_dataset_split_full = DatasetDict({\n",
    "    'train':      aapl_dataset_split_1['train'],\n",
    "    'test':       aapl_dataset_split_2['test'],\n",
    "    'validation': aapl_dataset_split_2['train']})\n",
    "\n",
    "aapl_dataset_split_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7975544-2773-4980-8eb0-6e38ec21ed69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'Thank you @Apple for fixing the #Swift sourcekit crashes in #XCode :). Life is better now! ',\n",
       " 'label': 2}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aapl_dataset_split_full['train'][50]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626b40e6-e199-4f7d-a310-8c5815ca64ed",
   "metadata": {},
   "source": [
    "Now as in the previous notebook, the DatasetDict object consists of three splits as Dataset objects. We once again load\n",
    "the tokenizer using AutoTokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54a6d4c7-f8ec-4be1-be01-579ab33837b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b4c00e2-10d7-4492-8f9d-2b28ef26b3dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [0, 47, 1600, 3345, 9646, 13545, 141, 6, 2307, 10638, 4, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer(\"The quick brown fox jumps over the lazy dog.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba58f3e1-17f6-44ce-aaca-0ef29338b020",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': Value(dtype='string', id=None),\n",
       " 'label': ClassLabel(num_classes=3, names=['negative', 'neutral', 'positive'], id=None)}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aapl_dataset_split_full['train'].features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790767e1-d227-4943-ab87-cb00d03d676b",
   "metadata": {},
   "source": [
    "Now we tokenize the entire dataset (all three splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e013484f-0008-4c65-b0fa-468ffecf341f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function tokenize_fn at 0x0000025198B83790> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d569704de9ba44efa34fa83eea6ab569",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0ex [00:00, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94dda2f2d5b04ded8c9c5176a485d8a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0ex [00:00, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8ca9c7540934aaa9e27c33400d29328",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0ex [00:00, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_fn(examples):\n",
    "    return tokenizer(examples['text'], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_aapl = aapl_dataset_split_full.map(tokenize_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919efb45-8a8b-4a4f-8407-5b76a6145ca2",
   "metadata": {},
   "source": [
    "Here we convert the tokenized splits into TensorFlow objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "38a667b3-a74c-4176-adbe-1c3a025f4b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.data.data_collator import tf_default_data_collator\n",
    "\n",
    "data_collator = tf_default_data_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5d3d8fbf-a5d5-45b1-a8ab-8a1c8232df96",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_train_dataset = tokenized_aapl['train'].to_tf_dataset(\n",
    "    columns=[\"attention_mask\", \"input_ids\", \"token_type_ids\"],\n",
    "    label_cols=[\"label\"],\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=8,\n",
    ")\n",
    "\n",
    "tf_validation_dataset = tokenized_aapl['validation'].to_tf_dataset(\n",
    "    columns=[\"attention_mask\", \"input_ids\", \"token_type_ids\"],\n",
    "    label_cols=[\"label\"],\n",
    "    shuffle=False,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=8,\n",
    ")\n",
    "\n",
    "tf_test_dataset = tokenized_aapl['test'].to_tf_dataset(\n",
    "    columns=[\"attention_mask\", \"input_ids\", \"token_type_ids\"],\n",
    "    label_cols=[\"label\"],\n",
    "    shuffle=False,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85027287-2579-4710-a6f6-47e0195829a4",
   "metadata": {},
   "source": [
    "Now we load the model we previously trained and proceed to train it on the AAPL data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dead5b71-a75b-4806-badb-d4ace0d22411",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at C:/Users/Kaya/Documents/capstone/models/bertweet_simple/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFAutoModelForSequenceClassification\n",
    "\n",
    "bertweet_simple = TFAutoModelForSequenceClassification.from_pretrained(\"./models/bertweet_simple/\", num_labels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8d6f30f8-61e0-4926-8a50-b0b07fa77998",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import create_optimizer\n",
    "\n",
    "batch_size = 8\n",
    "num_epochs = 3\n",
    "batches_per_epoch = len(tf_train_dataset) // batch_size\n",
    "total_train_steps = int(batches_per_epoch * num_epochs)\n",
    "optimizer, schedule = create_optimizer(init_lr=5e-6, num_warmup_steps=0, num_train_steps=total_train_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bf05d0a1-1579-4be3-b8ca-411a8e3892bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "bertweet_simple.compile(\n",
    "    optimizer = optimizer,\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics = [tf.metrics.SparseCategoricalAccuracy(), tf.keras.metrics.MeanAbsoluteError()],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bcc994aa-1d03-4eab-8709-5acaacfce64c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Kaya\\anaconda3\\envs\\capstone\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py:5043: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n",
      "53/53 [==============================] - 10s 77ms/step - loss: 0.7435 - sparse_categorical_accuracy: 0.6770 - mean_absolute_error: 1.8227\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.7434712052345276, 0.6770334839820862, 1.8227293491363525]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bertweet_simple.evaluate(tf_test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "31d86520-10b5-419e-b3e6-4f08c1a00819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "298/298 [==============================] - 88s 251ms/step - loss: 0.5074 - sparse_categorical_accuracy: 0.8058 - mean_absolute_error: 1.6279 - val_loss: 0.5078 - val_sparse_categorical_accuracy: 0.8034 - val_mean_absolute_error: 1.7611\n",
      "Epoch 2/3\n",
      "298/298 [==============================] - 75s 253ms/step - loss: 0.4830 - sparse_categorical_accuracy: 0.8087 - mean_absolute_error: 1.6249 - val_loss: 0.5078 - val_sparse_categorical_accuracy: 0.8034 - val_mean_absolute_error: 1.7611\n",
      "Epoch 3/3\n",
      "298/298 [==============================] - 76s 254ms/step - loss: 0.4918 - sparse_categorical_accuracy: 0.8037 - mean_absolute_error: 1.6197 - val_loss: 0.5078 - val_sparse_categorical_accuracy: 0.8034 - val_mean_absolute_error: 1.7611\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x252386ac9d0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bertweet_aapl = bertweet_simple\n",
    "\n",
    "bertweet_aapl.fit(tf_train_dataset, validation_data=tf_validation_dataset, epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fabd5f50-f7c3-48f2-a7b3-4a2274b91555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 4s 79ms/step - loss: 0.5041 - sparse_categorical_accuracy: 0.7823 - mean_absolute_error: 1.7455\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5040907263755798, 0.7822966575622559, 1.7455029487609863]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bertweet_aapl.evaluate(tf_test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f808c357-ce85-4c91-9e14-c39e92c4721d",
   "metadata": {},
   "source": [
    "The model from the first notebook (here called 'bertweet_simple' has an accuracy of 0.6770 on the test split, while the\n",
    "newly trained 'bertweet_aapl' model has accuracy of 0.7823."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8cf37eed-8450-443e-8747-05a9ffe4a41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bertweet_aapl.save_pretrained(\"./models/bertweet_aapl/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:capstone]",
   "language": "python",
   "name": "conda-env-capstone-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
